{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-12T09:31:45.998190Z",
     "start_time": "2024-08-12T09:31:45.992594Z"
    }
   },
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import torch\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import simpleaudio as sa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, M2M100ForConditionalGeneration, M2M100Tokenizer, SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from colorama import Fore, Style, init\n",
    "import gradio as gr\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:14:25.312913Z",
     "start_time": "2024-08-12T09:14:25.308876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize colorama\n",
    "init(autoreset=True)\n",
    "\n",
    "# Custom logger\n",
    "class ColoredLogger(logging.Logger):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def info(self, msg, *args, **kwargs):\n",
    "        super().info(f\"{Fore.GREEN}{msg}{Style.RESET_ALL}\", *args, **kwargs)\n",
    "\n",
    "    def warning(self, msg, *args, **kwargs):\n",
    "        super().warning(f\"{Fore.YELLOW}{msg}{Style.RESET_ALL}\", *args, **kwargs)\n",
    "\n",
    "    def error(self, msg, *args, **kwargs):\n",
    "        super().error(f\"{Fore.RED}{msg}{Style.RESET_ALL}\", *args, **kwargs)"
   ],
   "id": "9c3e544bb887d4b1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:14:25.318150Z",
     "start_time": "2024-08-12T09:14:25.314334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up logging\n",
    "logging.setLoggerClass(ColoredLogger)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_execution(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logger.info(f\"{Fore.CYAN}Starting: {func.__name__}{Style.RESET_ALL}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        logger.info(f\"{Fore.CYAN}Finished: {func.__name__}{Style.RESET_ALL}\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "id": "fb7f5a1fe5128c12",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:14:25.898750Z",
     "start_time": "2024-08-12T09:14:25.895226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AudioRecorder:\n",
    "    def __init__(self, rate=16000, channels=1):\n",
    "        self.rate = rate\n",
    "        self.channels = channels\n",
    "\n",
    "    @log_execution\n",
    "    def record(self, duration):\n",
    "        logger.info(f\"{Fore.BLUE}Recording for {duration} seconds...{Style.RESET_ALL}\")\n",
    "        recording = sd.rec(int(duration * self.rate), samplerate=self.rate, channels=self.channels)\n",
    "        sd.wait()\n",
    "        logger.info(f\"{Fore.BLUE}Recording finished{Style.RESET_ALL}\")\n",
    "        return recording\n",
    "\n",
    "    def save_audio(self, recording, filename):\n",
    "        sf.write(filename, recording, self.rate)\n",
    "        logger.info(f\"{Fore.BLUE}Audio saved as '{filename}'{Style.RESET_ALL}\")"
   ],
   "id": "f650058ba8352de7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:14:26.661607Z",
     "start_time": "2024-08-12T09:14:26.657989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpeechRecognizer:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"{Fore.MAGENTA}Initializing Whisper model on {self.device}{Style.RESET_ALL}\")\n",
    "        self.processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\").to(self.device)\n",
    "\n",
    "    @log_execution\n",
    "    def recognize(self, audio_file, language):\n",
    "        logger.info(f\"{Fore.MAGENTA}Recognizing speech in {language}{Style.RESET_ALL}\")\n",
    "        audio_input, sample_rate = sf.read(audio_file)\n",
    "        input_features = self.processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_features.to(self.device)\n",
    "        forced_decoder_ids = self.processor.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n",
    "        predicted_ids = self.model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "        transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "        return transcription[0]"
   ],
   "id": "a1e82212874df27d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:14:27.469746Z",
     "start_time": "2024-08-12T09:14:27.464584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Translator:\n",
    "    def __init__(self):\n",
    "        logger.info(f\"{Fore.YELLOW}Initializing M2M100 translation model{Style.RESET_ALL}\")\n",
    "        self.model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "        self.tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "\n",
    "    @log_execution\n",
    "    def translate(self, text, source_lang, target_lang):\n",
    "        logger.info(f\"{Fore.YELLOW}Translating from {source_lang} to {target_lang}{Style.RESET_ALL}\")\n",
    "        self.tokenizer.src_lang = source_lang\n",
    "        encoded_text = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        generated_tokens = self.model.generate(\n",
    "            **encoded_text, \n",
    "            forced_bos_token_id=self.tokenizer.get_lang_id(target_lang),\n",
    "            max_length=200,\n",
    "            num_beams=5,\n",
    "            length_penalty=0.8,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        translated = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "        return self.post_process_translation(translated)\n",
    "\n",
    "    def post_process_translation(self, text):\n",
    "        sentences = text.split('.')\n",
    "        unique_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if sentence and sentence not in unique_sentences:\n",
    "                sentence = sentence.capitalize()\n",
    "                if not sentence.endswith(('?', '!', '.')):\n",
    "                    sentence += '.'\n",
    "                unique_sentences.append(sentence)\n",
    "        return ' '.join(unique_sentences)"
   ],
   "id": "e93f506751572049",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:14:28.103243Z",
     "start_time": "2024-08-12T09:14:28.100438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextToSpeech:\n",
    "    def __init__(self):\n",
    "        logger.info(f\"{Fore.GREEN}Initializing SpeechT5 text-to-speech model{Style.RESET_ALL}\")\n",
    "        self.processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        self.model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "        self.vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "        self.speaker_embeddings = torch.randn(1, 512)\n",
    "\n",
    "    @log_execution\n",
    "    def synthesize(self, text):\n",
    "        logger.info(f\"{Fore.GREEN}Synthesizing speech{Style.RESET_ALL}\")\n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\")\n",
    "        speech = self.model.generate_speech(inputs[\"input_ids\"], self.speaker_embeddings, vocoder=self.vocoder)\n",
    "        return speech.numpy()\n",
    "\n",
    "    def adjust_speed(self, audio, speed_factor=0.8):\n",
    "        return np.interp(\n",
    "            np.arange(0, len(audio), speed_factor),\n",
    "            np.arange(0, len(audio)),\n",
    "            audio\n",
    "        ).astype(audio.dtype)"
   ],
   "id": "fab18ca2152a4c71",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:14:28.948162Z",
     "start_time": "2024-08-12T09:14:28.945509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def play_audio(audio_file):\n",
    "    wave_obj = sa.WaveObject.from_wave_file(audio_file)\n",
    "    play_obj = wave_obj.play()\n",
    "    play_obj.wait_done()"
   ],
   "id": "9ae02d482c76d17a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:14:29.781761Z",
     "start_time": "2024-08-12T09:14:29.775764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VoiceTranslator:\n",
    "    def __init__(self):\n",
    "        self.recorder = AudioRecorder()\n",
    "        self.recognizer = SpeechRecognizer()\n",
    "        self.translator = Translator()\n",
    "        self.tts = TextToSpeech()\n",
    "\n",
    "    def get_language(self, prompt):\n",
    "        while True:\n",
    "            lang = input(f\"{Fore.CYAN}{prompt}{Style.RESET_ALL}\").strip().lower()\n",
    "            if lang in ['en', 'es', 'fr', 'de', 'it', 'pt', 'ru', 'zh', 'ja', 'ko']:\n",
    "                return lang\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported language code: {lang}. Please try again.\")\n",
    "\n",
    "    def get_duration(self):\n",
    "        while True:\n",
    "            try:\n",
    "                duration = float(input(f\"{Fore.CYAN}Enter recording duration in seconds: {Style.RESET_ALL}\"))\n",
    "                if duration > 0:\n",
    "                    return duration\n",
    "                else:\n",
    "                    logger.warning(\"Duration must be positive. Please try again.\")\n",
    "            except ValueError:\n",
    "                logger.warning(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "    @log_execution\n",
    "    def translate_speech(self):\n",
    "        source_lang = self.get_language(\"Enter the source language code (e.g., 'en' for English): \")\n",
    "        target_lang = self.get_language(\"Enter the target language code (e.g., 'es' for Spanish): \")\n",
    "        duration = self.get_duration()\n",
    "\n",
    "        audio_data = self.recorder.record(duration)\n",
    "        self.recorder.save_audio(audio_data, \"input_audio.wav\")\n",
    "\n",
    "        text = self.recognizer.recognize(\"input_audio.wav\", language=source_lang)\n",
    "        logger.info(f\"{Fore.WHITE}Recognized text: {text}{Style.RESET_ALL}\")\n",
    "\n",
    "        if not text.strip():\n",
    "            logger.warning(\"No speech detected. Please try again.\")\n",
    "            return None\n",
    "\n",
    "        translated_text = self.translator.translate(text, source_lang, target_lang)\n",
    "        logger.info(f\"{Fore.WHITE}Translated text: {translated_text}{Style.RESET_ALL}\")\n",
    "\n",
    "        audio_output = self.tts.synthesize(translated_text)\n",
    "        audio_output_slowed = self.tts.adjust_speed(audio_output)\n",
    "        return audio_output_slowed"
   ],
   "id": "e085907c24a8233f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:16:24.859439Z",
     "start_time": "2024-08-12T09:15:26.670113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translator = VoiceTranslator()\n",
    "audio_output = translator.translate_speech()\n",
    "if audio_output is not None:\n",
    "    sf.write(\"translated_audio.wav\", audio_output, samplerate=16000)\n",
    "    logger.info(f\"{Fore.GREEN}Translation complete. Output saved as 'translated_audio.wav'{Style.RESET_ALL}\")\n",
    "    play_option = input(f\"{Fore.CYAN}Do you want to play the translated audio? (y/n): {Style.RESET_ALL}\").strip().lower()\n",
    "    if play_option == 'y':\n",
    "        play_audio(\"translated_audio.wav\")\n",
    "else:\n",
    "    logger.error(\"Translation failed. Please try again.\")"
   ],
   "id": "c60552e671f29b76",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 11:15:26,671 - __main__ - INFO - Initializing Whisper model on cpu\n",
      "2024-08-12 11:15:28,235 - __main__ - INFO - Initializing M2M100 translation model\n",
      "2024-08-12 11:15:29,642 - __main__ - INFO - Initializing SpeechT5 text-to-speech model\n",
      "2024-08-12 11:15:31,453 - __main__ - INFO - Starting: translate_speech\n",
      "2024-08-12 11:15:37,876 - __main__ - INFO - Starting: record\n",
      "2024-08-12 11:15:37,882 - __main__ - INFO - Recording for 5.0 seconds...\n",
      "2024-08-12 11:15:43,102 - __main__ - INFO - Recording finished\n",
      "2024-08-12 11:15:43,103 - __main__ - INFO - Finished: record\n",
      "2024-08-12 11:15:43,107 - __main__ - INFO - Audio saved as 'input_audio.wav'\n",
      "2024-08-12 11:15:43,109 - __main__ - INFO - Starting: recognize\n",
      "2024-08-12 11:15:43,110 - __main__ - INFO - Recognizing speech in es\n",
      "2024-08-12 11:16:05,951 - __main__ - INFO - Finished: recognize\n",
      "2024-08-12 11:16:05,954 - __main__ - INFO - Recognized text:  Buenos días.\n",
      "2024-08-12 11:16:05,955 - __main__ - INFO - Starting: translate\n",
      "2024-08-12 11:16:05,956 - __main__ - INFO - Translating from es to en\n",
      "2024-08-12 11:16:20,411 - __main__ - INFO - Finished: translate\n",
      "2024-08-12 11:16:20,414 - __main__ - INFO - Translated text: And good days.\n",
      "2024-08-12 11:16:20,415 - __main__ - INFO - Starting: synthesize\n",
      "2024-08-12 11:16:20,416 - __main__ - INFO - Synthesizing speech\n",
      "2024-08-12 11:16:22,312 - __main__ - INFO - Finished: synthesize\n",
      "2024-08-12 11:16:22,314 - __main__ - INFO - Finished: translate_speech\n",
      "2024-08-12 11:16:22,318 - __main__ - INFO - Translation complete. Output saved as 'translated_audio.wav'\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:31:57.057950Z",
     "start_time": "2024-08-12T09:31:57.050451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GradioInterface:\n",
    "    def __init__(self):\n",
    "        self.translator = VoiceTranslator()\n",
    "        self.languages = {\n",
    "            'English': 'en', 'Spanish': 'es', 'French': 'fr', 'German': 'de', \n",
    "            'Italian': 'it', 'Portuguese': 'pt', 'Russian': 'ru', 'Chinese': 'zh', \n",
    "            'Japanese': 'ja', 'Korean': 'ko'\n",
    "        }\n",
    "        logger.info(\"GradioInterface initialized\")\n",
    "        self.verify_models()\n",
    "\n",
    "    def verify_models(self):\n",
    "        logger.info(\"Verifying models...\")\n",
    "        assert self.translator.recognizer.model is not None, \"Speech recognition model not loaded\"\n",
    "        assert self.translator.translator.model is not None, \"Translation model not loaded\"\n",
    "        assert self.translator.tts.model is not None, \"Text-to-speech model not loaded\"\n",
    "        logger.info(\"All models verified successfully\")\n",
    "\n",
    "    def resample_audio(self, audio, orig_sr, target_sr=16000):\n",
    "        logger.info(f\"Resampling audio from {orig_sr} Hz to {target_sr} Hz\")\n",
    "        resampled = signal.resample(audio, int(len(audio) * target_sr / orig_sr))\n",
    "        return resampled\n",
    "\n",
    "    def translate(self, audio, source_lang, target_lang):\n",
    "        try:\n",
    "            if audio is None:\n",
    "                logger.warning(\"No audio detected\")\n",
    "                return \"No audio detected. Please try again.\", None\n",
    "\n",
    "            logger.info(f\"Translating from {source_lang} to {target_lang}\")\n",
    "            \n",
    "            # Remuestrear el audio a 16000 Hz\n",
    "            orig_sr = audio[0]\n",
    "            audio_data = audio[1]\n",
    "            if len(audio_data.shape) > 1:\n",
    "                audio_data = audio_data.mean(axis=1)  # Convertir a mono si es estéreo\n",
    "            if orig_sr != 16000:\n",
    "                audio_data = self.resample_audio(audio_data, orig_sr)\n",
    "\n",
    "            # Normalizar el audio\n",
    "            audio_data = audio_data / np.max(np.abs(audio_data))\n",
    "\n",
    "            # Guardar el audio remuestreado en un archivo temporal\n",
    "            temp_file = \"temp_input_audio.wav\"\n",
    "            sf.write(temp_file, audio_data, 16000)\n",
    "            logger.info(f\"Audio saved to {temp_file}\")\n",
    "\n",
    "            source_lang_code = self.languages[source_lang]\n",
    "            target_lang_code = self.languages[target_lang]\n",
    "            \n",
    "            # Reconocimiento de voz\n",
    "            text = self.translator.recognizer.recognize(temp_file, language=source_lang_code)\n",
    "            logger.info(f\"Recognized text: {text}\")\n",
    "            if not text.strip():\n",
    "                logger.warning(\"No speech detected in the audio\")\n",
    "                return \"No speech detected. Please try again.\", None\n",
    "\n",
    "            # Traducción\n",
    "            translated_text = self.translator.translator.translate(text, source_lang_code, target_lang_code)\n",
    "            logger.info(f\"Translated text: {translated_text}\")\n",
    "            \n",
    "            # Síntesis de voz\n",
    "            audio_output = self.translator.tts.synthesize(translated_text)\n",
    "            audio_output_slowed = self.translator.tts.adjust_speed(audio_output)\n",
    "            \n",
    "            # Guardar el audio de salida\n",
    "            output_file = \"translated_audio.wav\"\n",
    "            sf.write(output_file, audio_output_slowed, samplerate=16000)\n",
    "            logger.info(f\"Translated audio saved to {output_file}\")\n",
    "            \n",
    "            # Limpiar archivos temporales\n",
    "            os.remove(temp_file)\n",
    "            \n",
    "            return translated_text, (16000, audio_output_slowed)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred: {str(e)}\", exc_info=True)\n",
    "            return f\"An error occurred: {str(e)}\", None\n",
    "\n",
    "    def launch(self):\n",
    "        iface = gr.Interface(\n",
    "            fn=self.translate,\n",
    "            inputs=[\n",
    "                gr.Audio(type=\"numpy\", label=\"Input Audio\"),\n",
    "                gr.Dropdown(list(self.languages.keys()), label=\"Source Language\"),\n",
    "                gr.Dropdown(list(self.languages.keys()), label=\"Target Language\")\n",
    "            ],\n",
    "            outputs=[\n",
    "                gr.Textbox(label=\"Translated Text\"),\n",
    "                gr.Audio(type=\"numpy\", label=\"Translated Audio\")\n",
    "            ],\n",
    "            title=\"Voice Translator\",\n",
    "            description=\"Translate speech from one language to another.\"\n",
    "        )\n",
    "        iface.launch()"
   ],
   "id": "135465541d7dcf72",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:31:57.585048Z",
     "start_time": "2024-08-12T09:31:57.580168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Asegúrate de que los modelos se carguen correctamente\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "logger.info(\"Models loaded successfully\")"
   ],
   "id": "771202595d85568c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 11:31:57,581 - __main__ - INFO - Models loaded successfully\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T09:32:04.816949Z",
     "start_time": "2024-08-12T09:31:58.266470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Iniciar la interfaz de Gradio\n",
    "interface = GradioInterface()\n",
    "interface.launch()"
   ],
   "id": "bbf7210925ce809",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 11:31:58,267 - __main__ - INFO - Initializing Whisper model on cpu\n",
      "2024-08-12 11:32:01,284 - __main__ - INFO - Initializing M2M100 translation model\n",
      "/Users/adrianinfantes/Library/Caches/pypoetry/virtualenvs/traductoraivoz-HmbIT5aM-py3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-08-12 11:32:02,510 - __main__ - INFO - Initializing SpeechT5 text-to-speech model\n",
      "2024-08-12 11:32:04,701 - __main__ - INFO - GradioInterface initialized\n",
      "2024-08-12 11:32:04,702 - __main__ - INFO - Verifying models...\n",
      "2024-08-12 11:32:04,703 - __main__ - INFO - All models verified successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 11:32:04,802 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7865/startup-events \"HTTP/1.1 200 OK\"\n",
      "2024-08-12 11:32:04,810 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7865/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9dca36fb0dbda47b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
